# -*- coding: utf-8 -*-
"""embeddings using instruct and LLm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPsf60VgubfJE5mO--R7fDyh_rnb5ezG
"""

!pip -q install langchain  tiktoken  pypdf sentence_transformers InstructorEmbedding faiss-cpu

!pip install kaleido

!pip install python-multipart

!pip install cohere

!pip -q install langchain huggingface_hub transformers sentence_transformers

import os


os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader

# InstructorEmbedding
from InstructorEmbedding import INSTRUCTOR
from langchain.embeddings import HuggingFaceInstructEmbeddings



"""### Load Multiple files from Directory"""

# connect your Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive"

root_dir = "/content/gdrive/My Drive"
from langchain.document_loaders import TextLoader

loader = DirectoryLoader(f'{root_dir}/Documents/', glob="./*.txt", loader_cls=TextLoader)
documents = loader.load()



"""### Dividing in to the chucks for instruct embeddings max context limit of 512

"""

text_splitter = RecursiveCharacterTextSplitter(
                                               chunk_size=482,
                                               chunk_overlap=200)

texts = text_splitter.split_documents(documents)

texts[2]

len(texts)

"""### Get Embeddings for OUR Documents"""

!pip install faiss-cpu

import pickle
import faiss
from langchain.vectorstores import FAISS

def store_embeddings(docs, embeddings, sotre_name, path):

    vectorStore = FAISS.from_documents(docs, embeddings)

    with open(f"{path}/faiss_{sotre_name}.pkl", "wb") as f:
        pickle.dump(vectorStore, f)

def load_embeddings(sotre_name, path):
    with open(f"{path}/faiss_{sotre_name}.pkl", "rb") as f:
        VectorStore = pickle.load(f)
    return VectorStore





"""### HF Instructor Embeddings"""

from langchain.embeddings import HuggingFaceInstructEmbeddings

instructor_embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl",
                                                      model_kwargs={"device": "cuda"})

Embedding_store_path = f"{root_dir}/Embedding_store"

store_embeddings(texts,
                  instructor_embeddings,
                  sotre_name='instructEmbeddings',
                 path=Embedding_store_path)

db_instructEmbedd = load_embeddings(sotre_name='instructEmbeddings',
                                    path=Embedding_store_path)

#db_instructEmbedd = FAISS.from_documents(texts, instructor_embeddings)

retriever = db_instructEmbedd.as_retriever(search_kwargs={"k": 5})

retriever.search_type

retriever.search_kwargs





from langchain import PromptTemplate

def custom_prompt_template():
    prompt_template = """
Given the following information:

Context:{context}
Row heading: {row_heading}
Column heading: {column_heading}
Carefully search the corpus of scientific papers and provide a concise and accurate response to the following question:

What is the value or information relevant to {row_heading} and {column_heading}?
Strict adherence to factual accuracy is required. Do not create or fabricate any information. If no relevant information is found in the corpus, leave the response blank.
"""

    # Pass the prompt_template string directly to the PromptTemplate constructor
    prompt = PromptTemplate(template=prompt_template,
                             input_variables=['context','row_heading', 'column_heading'])
    return prompt

# Store the generated prompt object for later use
prompt = custom_prompt_template()



!pip install ctransformers[cuda]
from langchain.llms import CTransformers


    # Load the locally downloaded model here
llm = CTransformers(
    model = "TheBloke/Llama-2-7B-Chat-GGML",
    model_type="llama",
    max_new_tokens = 512,
)



qa_chain_instrucEmbed= RetrievalQA.from_chain_type(llm=llm,
                                      chain_type='stuff',
                                      retriever=retriever,
                                      return_source_documents=True,
                                      chain_type_kwargs={'prompt': prompt}
                                       )

## Cite sources

import textwrap

def wrap_text_preserve_newlines(text, width=200):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

def process_llm_response(llm_response):
    print(wrap_text_preserve_newlines(llm_response['result']))
    print('\nSources:')
    for source in llm_response["source_documents"]:
        print(source.metadata['source'])

!pip install pymongo
import pandas as pd



# Connect to MongoDB database
import pymongo
client = pymongo.MongoClient("mongodb+srv://intern:JeUDstYbGTSczN4r@interntest.i7decv0.mongodb.net/")
db = client.interntest  # Accessing the database
papers_collection = db.papers

def fill_excel_cells(excel_file_path):
    df = pd.read_excel(excel_file_path)

    for row_index, row in df.iterrows():
        for col_index, cell_value in enumerate(row):
            if pd.isna(cell_value):  # Check if the cell is empty
                row_heading = df.iloc[row_index, 0]
                col_heading = df.columns[col_index]
                query = f"{row_heading} {col_heading} "  # query creation

                try:
                    # Retrieve relevant documents from MongoDB
                    retrieved_documents = papers_collection.find({"transcription": {"$regex": query}})

                    # Constructed prompt for LLM based on query and retrieved documents
                    prompt = f"Answer the question: {query}\n\nRelevant context: {' '.join(doc['transcription'] for doc in retrieved_documents)}"

                    # Use RetrievalQA chain to get answer from LLM
                    llm_response = qa_chain_instrucEmbed(query=prompt)
                    extracted_info = llm_response['result']

                    # Fill the cell with the extracted information
                    df.iloc[row_index, col_index] = extracted_info
                except Exception as e:
                    print(f"Error filling cell ({row_index}, {col_index}): {e}")

    # Save the filled Excel file
    df.to_excel("output_excel.xlsx", index=False)

# Example usage
fill_excel_cells("/content/gdrive/MyDrive/Documents/excel.xlsx")